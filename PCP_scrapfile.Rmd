---
title: "PCP_scrapfile"
author: "Will P."
date: "3/26/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Scrap from PCP_alphaWorkflow.Rmd
```{r}
#<<<<<<< HEAD
#=======

routes$lagrangedistX <- predict(lagrangepoly, routes$distanceX) # predict X distances
routes$lagrangedistY <- predict(lagrangepoly, routes$distanceY) # predict Y distances

routes$distanceXsqrt <- (routes$distanceX)
routes$distanceYsqrt <- sqrt(routes$distanceY)

plot4a <- ggplot(routes, aes(
                routes$distanceXsqrt * longDiffSign, 
                routes$distanceYsqrt * latDiffSign, 
                color = routes$distance)) + 
            geom_point(stroke = 1, size = routes$DTWconnectivity) + 
            geom_point(data = (as.data.frame(ctrPt)), aes(0, 0), color = "orange") +
            coord_fixed() + 
            labs(color = "Great circle distance in km") + 
        xlim(sqrt(maxdist) * -1,sqrt(maxdist)) + 
        ylim(sqrt(maxdist) * -1,sqrt(maxdist))

plot08 #EM 3/29: getting most points removed in this version, but not the same prob in plot4

## 08: Limit Points
##* Calculate the limit points in each direction

# xlim(maxdist * -1,maxdist) + ylim(maxdist * -1,maxdist)

# + scale_x_continuous(breaks = seq(-3500, 3500, by = 500)) + scale_y_continuous(breaks = seq(-3500, 3500, by = 500)) 
# scale_x_continuous(breaks=c(-maxdist,-3000,-2000,-1000,0,1000,2000,3000,maxdist)) + scale_y_continuous(breaks=c(-maxdist,-3000,-2000,-1000,0,1000,2000,3000,maxdist))
# , size = routes$flightsDTW

# geosphere::geodesic
# calculate limit points in each direction

latLon_S <- geodesic(ctrPt, 180, (maxdist*1000), f=1/298.257223563)

latLon_N <- geodesic(ctrPt, 0, (maxdist*1000), f=1/298.257223563)

latLon_E  <- geodesic(ctrPt, 90, (maxdist*1000), f=1/298.257223563)

latLon_W  <- geodesic(ctrPt, 270, (maxdist*1000), f=1/298.257223563)

limPts <- data.frame(longitude = double(), latitude = double(), azimuth = numeric())
limPts <- rbind(latLon_N,latLon_S,latLon_E,latLon_W)
limPts <- as.data.frame(limPts)

# now we make a data.frame of all the geodesic results

xlim1 <- limPts$longitude[limPts$longitude == min(limPts$longitude)]
xlim2 <- limPts$longitude[limPts$longitude == max(limPts$longitude)]
ylim1 <- limPts$latitude[limPts$latitude == min(limPts$latitude)]
ylim2 <- limPts$latitude[limPts$latitude == max(limPts$latitude)]

# some algebra to get xlim of long via km maxdistX in other position
# ditto for ylim

# take the maxdist
earthrad <- 6371

# radius of earth in km: 6,371 km

pi2 <- 3.14159265358979323846
ctrYrad <- ((ctrY * pi2) / 180)
angdistY <- maxdist/earthrad # get angular distance in radians
# rad2deg <- function(rad) {(rad * 180) / (pi)}
# deg2rad <- function(deg) {(deg * pi) / (180)}

latSrad <- asin((sin(ctrYrad)*cos(angdistY))
             +(cos(ctrYrad)*sin(angdistY)*cos(180)))

latSrad <- asin((sin(ctrY)*cos(maxdist/earthrad) + cos(ctrY)*sin(maxdist/earthrad)*cos(180)))

latS <- ((latSrad * 180)/(pi2))

geodist(ctrY, ctrX, latS, ctrX, units = "km")
geodist(ctrY, ctrX, latS, ctrX, units = "km")

latN <- asin(sin(ctrY)*cos(maxdist/earthrad)+cos(ctrY)*sin(maxdist/earthrad))
latN

latNdist <- geodist(ctrY, ctrX, latN, ctrX, units = "km")
latNdist
maxdist


# X AND Y STUFF WE DON'T NEED ANYMORE

  # create a column in routes, fill with km east-west distances
routes$distanceX <- geodist(ctrY, ctrX, ctrY, routes$long, units = "km")  
  # create a column in routes, fill with km north-south distances
routes$distanceY <- geodist(ctrY, ctrX, routes$lat, ctrX, units = "km")  

routes$longDiff <- routes$long - ctrX # actual latitude difference, use for square plot later
routes$longDiffSign <- ifelse(routes$long - ctrX < 0, -1, 1)

routes$latDiff <- routes$lat - ctrY # actal longitude difference, use for square plot later
routes$latDiffSign <- ifelse(routes$lat - ctrY < 0 , -1, 1)

  maxdistX <- max(routes$distanceX) # longest east-west distance
  maxdistY <- max(routes$distanceY) # longest north-south distance
  maxdist <- max(routes$distanceX, routes$distanceY) # max between both axial distances
  maxXcoord <- routes$lat[routes$distanceX == maxdistX]
  maxYcoord <- routes$long[routes$distanceY == maxdistY]
  
  if (maxdistX > maxdistY) # figure out which direction the farthest point along
    # either the east-west or north-south axis is, store its bearing in text and num
    if (maxXcoord < ctrX)
      {maxdistBearing <- 270; maxbearing <- "west"}
    if (maxXcoord > ctrX)
      {maxdistBearing <- 90; maxbearing <- "east"}
  if (maxdistY > maxdistX)
    if (maxYcoord < ctrY)
      {maxdistBearing <- 180; maxbearing <- "south"}
    if (maxYcoord > ctrY)
      {maxdistBearing <- 0; maxbearing <- "north"}


# CODE STASH FOR POLAR COORDINATES
# for (row in 1:nrow(routes)) # USE TO RESET
#   {routes[row,"crtPtMathbearing"] <- 0}
# cart2pol(-2347.424429, -975.4223676, degrees = TRUE) # convert from cartesian coordinates to polar
# p2 <- c(-112.0120, 33.43430) # PHX (long then lat) TEST POINT 2 (P1 STORED ALREADY)
# bearing(ctrPt, p2, a=6378137, f=1/298.257223563) # great circle distance should be 2684
pol2cart(routes$distance[2], routes$crtPtMathbearing[2], degrees = TRUE)

pol2cart(2684, 202.5643, degrees = TRUE) # get from great circle distance and bearing back to cartesian coordinates

# variation based on ed williams for less-than-half-the-earth-distances
# ugh, I cleaned my environment and pi is still effed up. Here's a bunch of digits
pi <- 3.141592653589793238462643383279502884197169399375105820974944592307816406286 

# a different answer ~ -36S, that is also wrong
latS_alt <- asin(sin(ctrY)*cos(maxdist/earthrad)+cos(ctrY)*sin(maxdist/earthrad)*cos(2*pi))
    ifelse((cos(ctrY) == 0),
       lonS_alt <- ctrX,
       lonS_alt <- ((ctrX-asin(sin(2*pi)*sin(maxdist/earthrad)/cos(latS_alt)) + pi) %% pi) - pi
    )


# lat=asin(sin(lat1)*cos(d)+cos(lat1)*sin(d)*cos(tc))
#      IF (cos(lat)=0)
#         lon=lon1      // endpoint a pole
#      ELSE
     #    lon=mod(lon1-asin(sin(tc)*sin(d)/cos(lat))+pi,2*pi)-pi
     # ENDIF

# var δ = d/R;
# var Δφ = δ * Math.cos(θ);
# var φ2 = φ1 + Δφ;
# 
# var Δψ = Math.log(Math.tan(φ2/2+Math.PI/4)/Math.tan(φ1/2+Math.PI/4));
# var q = Math.abs(Δψ) > 10e-12 ? Δφ / Δψ : Math.cos(φ1); // E-W course becomes ill-conditioned with 0/0
# 
# var Δλ = δ*Math.sin(θ)/q;
# var λ2 = λ1 + Δλ;


#>>>>>>> cdbc1607200ad4fd66562bb2754a3d728e0f69d6
# mod() = %%
lonE = (ctrX+asin(sin(3*pi/2)*sin(maxdist/6366.565)/cos(ctrY))+pi %% 2*pi)-pi ## ARGH!

# lonE = mod(lon1+asin(sin(tc)*sin(d)/cos(lat))+pi,2*pi)-pi

latE <- 
  # E, W (tc=pi/2 or 3*pi/2): lat = asin(sin(lat1)*cos(d))

# GAH, not right!! ^^^

# N (tc=0): lat = asin(sin(lat1)*cos(d)+cos(lat1)*sin(d))
#               = asin(sin(lat1 + d)
#               = lat1 + d

    # cheating example
#           N (tc=0): lat = asin(sin(lat1)*cos(d)+cos(lat1)*sin(d))
#               = asin(sin(lat1 + d)
#               = lat1 + d
# 
# S (tc=pi): lat = asin(sin(lat1)*cos(d)-cos(lat1)*sin(d))
#                = asin(sin(lat1 - d)
#                = lat1 - d
# 
# E, W (tc=pi/2 or 3*pi/2): lat = asin(sin(lat1)*cos(d))


# The circle example

    # added pkg sf

# sfdf <- st_sfc(st_point(cbind(ctrY, ctrX)),crs=4326)
# df_sf_buff <- st_buffer(sfdf, (maxdist*10)) #meters*10
# plot(df_sf_buff) 
# points(routes$long, routes$lat)

# circle in ggplot
ggplot(routes, aes(routes$long, routes$lat, color = routes$distance)) + geom_point() + theme_bw()
  
  ggplot((as.data.frame(ctrPt)), aes(ctrPt[1], ctrPt[2])) + geom_point(color = "Orange")
  # then would add a big buffer. Gah.
```


## 09E: Other Scale changes
```{r}
plot2 <- ggplot(routes, aes(routes$long, routes$lat, color = routes$distance)) + geom_point() + geom_point(data = (as.data.frame(ctrPt)), aes(ctrPt[1], ctrPt[2]), color = "orange")

# this doesn't work (produces NaNs or infinites), but sets a basic structure
plot2 + scale_x_log10(limits = c(xlim1,xlim2)) + scale_y_log10(c(ylim1,ylim2))

# really, we need a radial log scale, which would create a different structure than perhaps ggplot2 can offer

# we need a way of fitting a distance decay function (or is this an inverse distance decay function technically?) to the actual data, finding what's appropriate for the patterns you're trying to show in any given set.

# experimenting with a few basic ones for the test data, a square root function looks decent. and for people who are stats/quantitative geography junkies they might already know what distance decay suits their data based on previous literature (see the Taylor piece I added to the GitHub repo for some of the space cadet thoughts on this in geography). but what if there was a more custom solution that used math, but didn't really rely on our users knowing any? CATEGORICAL MAPPING (with a great R package name: catmaps), or PSEUDOSPATIAL CHARTS.

# two ideas:

# A. user-defined control points for the decay function. we say that 0 is the origin and 1200 pixels/units is maxdist (or whatever; i chose a number divisible by three for purposes of illustration, ggplot resizes dynamically but you get the point), and since we're trying to show things that are roughly (i) close, (ii) medium, and (iii) far; ask the user: how far away would something need to be from the origin to still be considered "close" in this data set? how far away would it need to get to be considered "far"? if the user says that airports within 500 miles are close, then 500 miles of distance in any direction would give you 400 pixels/units from the origin. if they say that over 2000 miles would be far, then that tells us that 800 pixels/units from the origin is equivalent to 2000 miles of real distance.

# so now we have the following four points and we need to find the smoothest function we can to fit them: (0,0), (500,400), (2000,800), (maxdist,1200). sounds like a regression problem! which R has been known to handle once or twice before. once we have the equation figured out, we just send all our distances through it. again, radial distances and bearings would be ideal since this is another potentially earth-distorting step, but that's okay for now. replot and PROFIT.

# B. data-driven approach using a similar approach to choropleth classes; obviously equal interval is boring, but we could define classes using natural breaks or some other clustering algorithm. does this get us closer to a liquid resize? maybe, but as long as it's labeled, is it still workable?

```